{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "import json\n",
    "\n",
    "rf = Roboflow(api_key=\"6Y4RTIEZtXjfjclMhIx1\")\n",
    "project = rf.workspace(\"ki-bu8lz\").project(\"ki-dybmq\")\n",
    "version = project.version(7)\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Das aktuelle Arbeitsverzeichnis ermitteln\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# Pfade für Trainings- und Validierungsordner definieren\n",
    "input_path_train = os.path.join(dir_path, f\"KI-{version.version}\", \"train\")\n",
    "input_path_valid = os.path.join(dir_path, f\"KI-{version.version}\", \"valid\")  # Rechtschreibung korrigiert: 'valid' statt 'vaild'\n",
    "output_path_train = os.path.join(dir_path, \"dataset_project\", \"images\", \"train\") \n",
    "output_path_valid = os.path.join(dir_path, \"dataset_project\", \"images\", \"valid\")  # Rechtschreibung korrigiert: 'valid' statt 'vaild'\n",
    "\n",
    "# Funktion zum Kopieren der Bilder und JSON-Dateien\n",
    "def copy_files(source_folder, target_folder):\n",
    "    # Erstellen des Zielordners, falls er nicht existiert\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    \n",
    "    # Liste der Dateien im Quellordner abrufen\n",
    "    file_names = os.listdir(source_folder)\n",
    "    \n",
    "    # Kopieren der Dateien (Bilder und JSON-Dateien)\n",
    "    for filename in file_names:\n",
    "        source_path = os.path.join(source_folder, filename)\n",
    "        target_path = os.path.join(target_folder, filename)\n",
    "        \n",
    "        # Wenn es sich um eine Bilddatei oder eine JSON-Datei handelt, kopieren\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.json')):\n",
    "            try:\n",
    "                shutil.copy(source_path, target_path)\n",
    "                print(f\"Datei '{filename}' wurde nach '{target_path}' kopiert.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Datei '{filename}' wurde nicht gefunden: {source_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Kopieren von '{filename}': {e}\")\n",
    "\n",
    "# Trainingsbilder und Annotationsdateien kopieren\n",
    "copy_files(input_path_train, output_path_train)\n",
    "\n",
    "# Validierungsbilder und Annotationsdateien kopieren\n",
    "copy_files(input_path_valid, output_path_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Basisordner\n",
    "output_path = os.path.join(dir_path, \"dataset_project\")\n",
    "input_path = os.path.join(\"KI-\" + version.version)\n",
    "\n",
    "# Eingabeordner\n",
    "train_images_folder = os.path.join(output_path, \"images\", \"train\")\n",
    "annotations_file = os.path.join(train_images_folder, \"_annotations.coco.json\")\n",
    "\n",
    "# Albumentations-Transformationspipeline\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.7),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.5, p=1.0),  # Kontrast und Helligkeit verändern\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids'])  # Bounding Box Transformation\n",
    ")\n",
    "\n",
    "# Anzahl der augmentierten Bilder pro Originalbild\n",
    "augmentation_count = 3\n",
    "\n",
    "# Lade die Annotationen\n",
    "with open(annotations_file, 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "# Erstelle ein Mapping der Bild-ID auf die Annotationen\n",
    "image_annotations_map = {}\n",
    "for ann in annotations_data['annotations']:\n",
    "    if ann['image_id'] not in image_annotations_map:\n",
    "        image_annotations_map[ann['image_id']] = []\n",
    "    image_annotations_map[ann['image_id']].append(ann)\n",
    "\n",
    "# Erstelle ein Mapping der Bild-ID auf den Dateinamen\n",
    "image_id_to_filename = {image['id']: image['file_name'] for image in annotations_data['images']}\n",
    "\n",
    "# Aktualisiere die zentrale JSON-Datei\n",
    "all_annotations = annotations_data\n",
    "\n",
    "# Augmentierung der Bilder\n",
    "for image_file in tqdm(os.listdir(train_images_folder)):\n",
    "    if not image_file.endswith((\".jpg\", \".png\", \".jpeg\")):  # Nur Bilddateien bearbeiten\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(train_images_folder, image_file)\n",
    "\n",
    "    # Hole die Bild-ID aus der Zuordnung\n",
    "    image_id = None\n",
    "    for img_id, filename in image_id_to_filename.items():\n",
    "        if filename == image_file:\n",
    "            image_id = img_id\n",
    "            break\n",
    "    if image_id is None:\n",
    "        print(f\"Keine Bild-ID gefunden für Bild: {image_file}\")\n",
    "        continue\n",
    "\n",
    "    # Bild laden\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Bild konnte nicht geladen werden: {image_file}\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Konvertierung von BGR zu RGB\n",
    "\n",
    "    # Hole die zugehörigen Annotationen\n",
    "    annotations = image_annotations_map.get(image_id, [])\n",
    "\n",
    "    # Bounding Boxen und Labels extrahieren\n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    for ann in annotations:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        bboxes.append([x, y, x + w, y + h])  # Umwandlung in [xmin, ymin, xmax, ymax]\n",
    "        category_ids.append(ann['category_id'] - 1)  # YOLO Labels beginnen bei 0\n",
    "\n",
    "    # Augmentierung durchführen\n",
    "    for i in range(augmentation_count):\n",
    "        augmented = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "        aug_image = augmented[\"image\"]\n",
    "        aug_bboxes = augmented[\"bboxes\"]\n",
    "\n",
    "        # Umwandlung des Tensors in ein NumPy-Array\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()  # Ändert (C, H, W) zu (H, W, C)\n",
    "        aug_image = np.clip(aug_image, 0, 255).astype(np.uint8)  # Sicherstellen, dass Werte zwischen 0 und 255 sind\n",
    "\n",
    "        # Speichern des augmentierten Bildes\n",
    "        aug_image_filename = f\"aug_{i}_{image_file}\"\n",
    "        aug_image_path = os.path.join(train_images_folder, aug_image_filename)\n",
    "        aug_image = cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)  # Zurück zu BGR für OpenCV\n",
    "        cv2.imwrite(aug_image_path, aug_image)\n",
    "\n",
    "        # Speichern der Annotationen für das augmentierte Bild\n",
    "        aug_annotations = []\n",
    "        for bbox, category_id in zip(aug_bboxes, category_ids):\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            w = xmax - xmin\n",
    "            h = ymax - ymin\n",
    "            aug_annotations.append({\n",
    "                'category_id': category_id + 1,  # Wieder zurück zu originalem Format (1-basierend)\n",
    "                'bbox': [xmin, ymin, w, h],\n",
    "                'image_id': len(all_annotations['images']) + 1,\n",
    "                'id': len(all_annotations['annotations']) + 1\n",
    "            })\n",
    "\n",
    "        # Füge die augmentierten Annotationen hinzu\n",
    "        all_annotations['annotations'].extend(aug_annotations)\n",
    "\n",
    "        # Füge die augmentierten Bildinformationen hinzu\n",
    "        all_annotations['images'].append({\n",
    "            'id': len(all_annotations['images']) + 1,\n",
    "            'file_name': aug_image_filename,\n",
    "            'height': image.shape[0],\n",
    "            'width': image.shape[1]\n",
    "        })\n",
    "\n",
    "# Überschreibe die zentrale JSON-Datei im selben Ordner\n",
    "with open(annotations_file, 'w') as json_file:\n",
    "    json.dump(all_annotations, json_file)\n",
    "\n",
    "print(\"Augmentierung abgeschlossen! Alle Dateien und die aktualisierte Annotation befinden sich im selben Ordner.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "output_path = os.path.join(dir_path, \"dataset_project\")\n",
    "\n",
    "# Angenommene Pfade für die JSON-Daten\n",
    "file_path_train = os.path.join(output_path, \"images\", \"train\", \"_annotations.coco.json\")\n",
    "file_path_valid = os.path.join(output_path, \"images\", \"valid\", \"_annotations.coco.json\")\n",
    "\n",
    "# Ausgabeordner für die Labels\n",
    "output_path = os.path.join(output_path)\n",
    "train_images_folder = os.path.join(output_path, \"images\", \"train\")\n",
    "valid_images_folder = os.path.join(output_path, \"images\", \"valid\")\n",
    "\n",
    "# Annahme: Die Daten werden aus einer bereits geladenen COCO JSON-Datei extrahiert\n",
    "with open(file_path_train, 'r') as f:\n",
    "    data_train = json.load(f)\n",
    "\n",
    "with open(file_path_valid, 'r') as f:\n",
    "    data_valid = json.load(f)\n",
    "\n",
    "# Funktion zum Abrufen der Annotationen für ein Bild\n",
    "def get_img_ann(image_id, data):\n",
    "    img_ann = []\n",
    "    isFound = False\n",
    "    for ann in data['annotations']:  # Verwendung des entsprechenden Datensatzes\n",
    "        if ann['image_id'] == image_id:\n",
    "            img_ann.append(ann)\n",
    "            isFound = True\n",
    "    return img_ann if isFound else None\n",
    "\n",
    "# Funktion zum Abrufen der Bildinformationen anhand des Dateinamens\n",
    "def get_img(filename, data):\n",
    "    for img in data['images']:  # Verwendung des entsprechenden Datensatzes\n",
    "        if img['file_name'] == filename:\n",
    "            return img\n",
    "    return None\n",
    "\n",
    "# Definieren des Ausgabeordners für die Labels\n",
    "labels_dir_train = os.path.join(output_path, \"labels\", \"train\")\n",
    "labels_dir_valid = os.path.join(output_path, \"labels\", \"valid\")\n",
    "\n",
    "if not os.path.exists(labels_dir_train):\n",
    "    os.makedirs(labels_dir_train)  # Sicherstellen, dass der Ordner existiert\n",
    "\n",
    "if not os.path.exists(labels_dir_valid):\n",
    "    os.makedirs(labels_dir_valid)  # Sicherstellen, dass der Ordner existiert\n",
    "\n",
    "# Bilddateinamen aus den Trainings- und Validierungsordnern abrufen\n",
    "file_names_train = os.listdir(train_images_folder)  # Namen der Trainingsbilder\n",
    "file_names_valid = os.listdir(valid_images_folder)  # Namen der Validierungsbilder\n",
    "\n",
    "# Zähler für die Bilder\n",
    "count = 0\n",
    "\n",
    "# Funktion, um Label-Dateien zu erstellen\n",
    "def process_images(file_names, images_folder, labels_dir, data):\n",
    "    global count\n",
    "    for filename in file_names:\n",
    "        print(f\"Verarbeite Bild: {filename}\")\n",
    "\n",
    "        # Bildinformationen abrufen\n",
    "        img = get_img(filename, data)\n",
    "        \n",
    "        if img:\n",
    "            img_id = img['id']\n",
    "            img_w = img['width']\n",
    "            img_h = img['height']\n",
    "\n",
    "            # Annotationen für dieses Bild abrufen\n",
    "            img_ann = get_img_ann(img_id, data)\n",
    "\n",
    "            if img_ann:\n",
    "                # Label-Datei im YOLO-Format erstellen\n",
    "                label_file_path = os.path.join(labels_dir, filename.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "                print(f\"Erstelle Label-Datei: {label_file_path}\")\n",
    "                with open(label_file_path, \"a\") as file_object:\n",
    "                    for ann in img_ann:\n",
    "                        current_category = ann['category_id'] - 1  # YOLO-Label beginnen bei 0\n",
    "                        current_bbox = ann['bbox']\n",
    "                        x = current_bbox[0]\n",
    "                        y = current_bbox[1]\n",
    "                        w = current_bbox[2]\n",
    "                        h = current_bbox[3]\n",
    "\n",
    "                        # Berechnung der Mittelpunkte\n",
    "                        x_centre = (x + (x + w)) / 2\n",
    "                        y_centre = (y + (y + h)) / 2\n",
    "\n",
    "                        # Normalisierung der Werte\n",
    "                        x_centre = x_centre / img_w\n",
    "                        y_centre = y_centre / img_h\n",
    "                        w = w / img_w\n",
    "                        h = h / img_h\n",
    "\n",
    "                        # Formatieren auf 6 Dezimalstellen\n",
    "                        x_centre = format(x_centre, '.6f')\n",
    "                        y_centre = format(y_centre, '.6f')\n",
    "                        w = format(w, '.6f')\n",
    "                        h = format(h, '.6f')\n",
    "\n",
    "                        # Schreiben der Annotation ins Label-File\n",
    "                        file_object.write(f\"{current_category} {x_centre} {y_centre} {w} {h}\\n\")\n",
    "\n",
    "                count += 1  # Zähler für Bildnummer erhöhen\n",
    "            else:\n",
    "                print(f\"Keine Annotationen für das Bild: {filename}\")\n",
    "        else:\n",
    "            print(f\"Bild nicht gefunden: {filename}\")\n",
    "\n",
    "# Verarbeite sowohl Trainings- als auch Validierungsbilder\n",
    "print(\"Verarbeite Trainingsbilder...\")\n",
    "process_images(file_names_train, train_images_folder, labels_dir_train, data_train)\n",
    "\n",
    "print(\"Verarbeite Validierungsbilder...\")\n",
    "process_images(file_names_valid, valid_images_folder, labels_dir_valid, data_valid)\n",
    "\n",
    "print(\"Alle Annotationen wurden verarbeitet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Verzeichnis für die Ausgabedaten\n",
    "output_path = os.path.join(dir_path, \"dataset_project\")\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# Eingabepfade für COCO JSON-Dateien\n",
    "file_path_train = os.path.join(output_path, \"images\", \"train\", \"_annotations.coco.json\")\n",
    "file_path_valid = os.path.join(output_path, \"images\", \"valid\", \"_annotations.coco.json\")\n",
    "\n",
    "# Verzeichnisse für die Trainings- und Validierungsbilder und Labels\n",
    "yaml_img_train = os.path.join(output_path, \"images\", \"train\")\n",
    "yaml_img_val = os.path.join(output_path, \"images\", \"valid\")\n",
    "yaml_label_train = os.path.join(output_path, \"labels\", \"train\")\n",
    "yaml_label_valid = os.path.join(output_path, \"labels\", \"valid\")\n",
    "\n",
    "# JSON-Dateien laden\n",
    "with open(file_path_train, 'r') as f:\n",
    "    data_train = json.load(f)\n",
    "\n",
    "with open(file_path_valid, 'r') as f:\n",
    "    data_valid = json.load(f)\n",
    "\n",
    "# Kategorien extrahieren\n",
    "category_names = [category['name'] for category in data_train['categories']]\n",
    "\n",
    "# Erstellen der YAML-Datenstruktur\n",
    "yaml_data = {\n",
    "    \"train\": './images/train',         # Pfad zu den Trainingsbildern\n",
    "    \"val\": './images/valid',           # Pfad zu den Validierungsbildern\n",
    "    \"train_labels\": './labels/train',  # Pfad zu den Trainingslabels\n",
    "    \"val_labels\": './labels/valid',    # Pfad zu den Validierungslabels\n",
    "    \"nc\": len(category_names),        # Anzahl der Kategorien\n",
    "    \"names\": category_names,          # Liste der Kategorienamen\n",
    "    \"batch\": 8                         # Batch-Größe hinzufügen (z.B. 8)\n",
    "}\n",
    "\n",
    "# Pfad zur YAML-Datei\n",
    "yaml_file_path = os.path.join(output_path, 'annotations.yaml')\n",
    "\n",
    "# YAML-Datei speichern\n",
    "with open(yaml_file_path, 'w') as yaml_file:\n",
    "    yaml.dump(yaml_data, yaml_file, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"YAML-Datei wurde erfolgreich gespeichert: {yaml_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#print(\"CUDA verfügbar:\", torch.cuda.is_available())\n",
    "#print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "#print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Keine GPU gefunden\")\n",
    "torch.cuda.device_count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import requests\n",
    "import os\n",
    "import torch\n",
    "print(\"CUDA verfügbar:\", torch.cuda.is_available())\n",
    "\n",
    "output_path = os.path.join(dir_path, \"dataset_project\")\n",
    "data_path = os.path.join(output_path, \"annotations.yaml\")\n",
    "\n",
    "# YOLOv8-Modell laden\n",
    "model = YOLO(\"yolov8n.pt\")  # Modell: n (Nano), s (Small), m (Medium), etc.\n",
    "\n",
    "# Training starten\n",
    "model.train(\n",
    "    data=(data_path),  # Pfad zur YAML-Datei\n",
    "    epochs=1,            # Anzahl der Epochen\n",
    "    imgsz=640,             # Bildgröße (anpassen bei Bedarf)\n",
    ")\n",
    "\n",
    "print(\"Training abgeschlossen!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL des Bildes\n",
    "image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQXWZd5b-rXfKMONr01EAgXsTZ0Sm_BMRAXvQ&s\"\n",
    "\n",
    "# Lokale Datei speichern\n",
    "local_image_path = \"downloaded_image.jpg\"\n",
    "response = requests.get(image_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Bild lokal speichern\n",
    "    with open(local_image_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Bild erfolgreich heruntergeladen: {local_image_path}\")\n",
    "else:\n",
    "    print(f\"Fehler beim Herunterladen des Bildes: {response.status_code}\")\n",
    "\n",
    "# Trainiertes YOLO-Modell laden\n",
    "model = YOLO(r\"C:\\Users\\thore\\Documents\\GitHub\\ki\\runs\\detect\\train12\\weights\\best.pt\")\n",
    "\n",
    "# Inferenz auf dem heruntergeladenen Bild\n",
    "results = model(local_image_path)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "# results ist eine Liste von Resultaten, daher durchlaufen wir sie.\n",
    "for result in results:\n",
    "    result.show()  # Zeigt das Bild mit erkannten Objekten\n",
    "\n",
    "# Ergebnisse speichern\n",
    "for result in results:\n",
    "    result.save(\"predictions/\")  # Speichert die analysierten Bilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the YOLOv8 model (choose the pre-trained model you want)\n",
    "model = torch.hub.load('ultralytics/yolov8', 'yolov8n')  # Replace 'yolov8n' with your desired model, such as 'yolov8s', 'yolov8m', or 'yolov8l'\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'input_video.mp4'  # Path to your input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the output video file name\n",
    "output_path = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform object detection on the current frame\n",
    "    results = model(frame)  # Perform detection on the current frame\n",
    "\n",
    "    # Render the results on the frame\n",
    "    frame = results.render()[0]  # results.render() modifies the frame\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame (optional)\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Press 'q' to quit early\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video objects\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Object detection complete. Output saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
